# -*- coding: utf-8 -*-
"""housing_price_prediction_with_ridge_and_lasso_regularisation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ez4KI-KVzpwpLk2ZXTx6_XoWHKAHsi6c

## Steps to follow
1. Reading, understanding and visualising the data.
2. Preparing the data for modeling (train-test split, rescalling etc)
3. Building the model
    - Training the model
    - Testing the model
4. Predictions and evaluation on the test set
"""
###############################
##Step: Data Cleanisation
###############################


# Importing pandas, numpy and plotting libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
pd.set_option('display.max_columns', None)
# %matplotlib inline

"""# Step 1:- Reading and understanding the data"""

# Read the data
f=r'/content/train.csv'
df = pd.read_csv(f)
df.head()
df.describe()
df.shape
df.info()

"""##### Creating a derivied column - "AgeHouse"
It is the age of the house calculated by `YrSold-YearBuilt`.
"""

# Age of the house 
df['AgeHouse'] = df['YrSold'] - df['YearBuilt']

df.head()

"""###### Removing few columns :-
1. YrSold and YearBuilt columns. As we derived the age of the house, so we do not need these columns anymore in our analysis.
2. YearRemodAdd, GarageYrBlt and MoSold columns - As these columns will not be such useful while doing our analysis. 
"""

# Removing columns
df.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'MoSold'], axis=1, inplace=True)

"""###### Converting numeric to categorical column
`OverallQual`, `OverallCond` and `MSSubClass` are categorical column but in the original dataset the types of these columns are numeric. Hence converting these columns to categorical. 
"""

# Converting column type for categorical variable from numeric to object
df['MSSubClass'] = df['MSSubClass'].astype('object')
df['OverallQual'] = df['OverallQual'].astype('object')
df['OverallCond'] = df['OverallCond'].astype('object')

df.info()

df.describe()

"""### Handaling missing values

#### Replacing few NA values to relevant names
We can see from the data dictonary that for few columns the NA value represents the `not present of the feature` in the house instead of representing the missing values. Hence, we need to impute the NA or the missing values for those columns to the relevant name.
"""

df['Alley'] = df['Alley'].replace(np.nan, 'No Alley')
df['BsmtQual'] = df['BsmtQual'].replace(np.nan, 'No Basement')
df['BsmtCond'] = df['BsmtCond'].replace(np.nan, 'No Basement')
df['BsmtExposure'] = df['BsmtExposure'].replace(np.nan, 'No Basement')
df['BsmtFinType1'] = df['BsmtFinType1'].replace(np.nan, 'No Basement')
df['BsmtFinType2'] = df['BsmtFinType2'].replace(np.nan, 'No Basement')
df['FireplaceQu'] = df['FireplaceQu'].replace(np.nan, 'No Fireplace')
df['GarageType'] = df['GarageType'].replace(np.nan, 'No Garage')
df['GarageFinish'] = df['GarageFinish'].replace(np.nan, 'No Garage')
df['GarageQual'] = df['GarageQual'].replace(np.nan, 'No Garage')
df['GarageCond'] = df['GarageCond'].replace(np.nan, 'No Garage')
df['PoolQC'] = df['PoolQC'].replace(np.nan, 'No Pool')
df['Fence'] = df['Fence'].replace(np.nan, 'No Fence')
df['MiscFeature'] = df['MiscFeature'].replace(np.nan, 'No MiscFeature')

"""#### Treating Missing Values in columns"""

# Checking percentage of missing values in columns
(round(100*(df.isnull().sum()/len(df.index)),2)).to_frame('Nulls').sort_values(by='Nulls' , ascending=False)

"""##### LotFrontage"""

# We can see that LotFrontage column has 17.74% missing values
# Let's see the distribution of the value of the LotFrontage column
df['LotFrontage'].describe()

"""We can see that mean and median are almost same (70) for LotFrontage column. Let us impute the missing values for LotFrontage with median because the column may have few outliers."""

# Imputing missing values of LotFrontage with median
df.loc[np.isnan(df['LotFrontage']), 'LotFrontage'] = df['LotFrontage'].median()

"""##### MasVnrType"""

# Treating the missing values for MasVnrArea column
df['MasVnrArea'].describe()

"""We can see that there is a huge range between the max and min value of MasVnrArea column. Also, the standard deviation is high. So, it is better to get rid of the records, which have missing values for MasVnrArea."""

# Deleting the rows for missing values in MasVnrArea
df = df[~np.isnan(df['MasVnrArea'])]

"""###### Electrical"""

# Treating missing values for Electrical column
# Let us check the count for values in Electrical 
df['Electrical'].value_counts()

"""We can see that the mode of the Electrical column is SBrkr as it has higher number of values. Lets impute the missing values in Electrical column with SBrkr."""

# Imputing Electrical column missing values with SBrkr
df.loc[pd.isnull(df['Electrical']), ['Electrical']] = 'SBrkr'

# Lets again check the percentage of missing values in columns
(round(100*(df.isnull().sum()/len(df.index)),2)).to_frame('Nulls').sort_values(by='Nulls' , ascending=False)

"""We can see that there is no missing values in any column now.

### Outliers treatment
"""

# Numeric columns list
numeric_cols = list(df.select_dtypes(include=['int64', 'float64']).columns)
# Removing "Id" column
numeric_cols.remove('Id')
print(numeric_cols)

# Plotting the spread of the numeric columns
plt.figure(figsize=(20,30))
for i in enumerate(numeric_cols):
    plt.subplot(8,4,i[0]+1)
    sns.boxplot(x=i[1], data=df)

"""We can see that there are outliers in few numeric columns.

### Checking Data Imbalance

#### Count the values of all the categorical variables
"""

# List of categorical columns
category_cols = df.select_dtypes(include='object').columns
category_cols

# Value count for each categorical column
for col in list(category_cols):
    print(df[col].value_counts())

"""#### Dropping few coulms having very lesser types of values
These column may bring very less variance in the data. Hence it is better to remove these columns.
"""

df.drop(['Street','Alley','LandContour','Utilities','LandSlope','Condition2','BldgType','RoofMatl','BsmtCond','Heating',
         'CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','MiscFeature','SaleType','ExterCond','BsmtFinType2'],
       axis=1, inplace=True)

"""#### Doing the following
1. Combining the lower number of value counts into a new value called "Other".
2. Creating the dummy variables for the categorical variables with multiple levels.
3. Dropping the dummy column, which has very low number of values (E.g.:-Other)

###### MSZoning
"""

df = df.replace({'MSZoning': {'C (all)':'Other', 'RH':'Other', 'FV':'Other'}})

df['MSZoning'].value_counts()

# Creating dummy variable
mz = pd.get_dummies(df['MSZoning'], prefix = 'MSZoning')
# Dropping MSZoning_Other column
mz = mz.drop('MSZoning_Other', axis=1)
# Adding result to master df
df = pd.concat([df, mz], axis=1)
# Dropping original variable
df = df.drop('MSZoning', axis=1)

"""##### LotShape"""

df = df.replace({'LotShape': {'IR2':'Other', 'IR3':'Other'}})

df['LotShape'].value_counts()

# Creating dummy variable
ls = pd.get_dummies(df['LotShape'], prefix = 'LotShape')
# Dropping Other column
ls = ls.drop('LotShape_Other', axis=1)
# Adding result to master df
df = pd.concat([df, ls], axis=1)
# Dropping original variable
df = df.drop('LotShape', axis=1)

"""##### LotConfig"""

df = df.replace({'LotConfig': {'CulDSac':'Other', 'FR2':'Other', 'FR3':'Other'}})

df['LotConfig'].value_counts()

# Creating dummy variable
lc = pd.get_dummies(df['LotConfig'], prefix = 'LotConfig')
# Dropping Other column
lc = lc.drop('LotConfig_Other', axis=1)
# Adding result to master df
df = pd.concat([df, lc], axis=1)
# Dropping original variable
df = df.drop('LotConfig', axis=1)

"""##### Neighborhood"""

df = df.replace({'Neighborhood': {'Blueste':'Other', 'NPkVill':'Other', 'Veenker':'Other', 'BrDale':'Other', 'Blmngtn':'Other',
                                 'MeadowV':'Other', 'SWISU':'Other', 'StoneBr':'Other', 'ClearCr':'Other',
                                 'IDOTRR':'Other', 'Timber':'Other', 'NoRidge':'Other', 'Mitchel':'Other'}})

# Creating dummy variable
nh = pd.get_dummies(df['Neighborhood'], prefix = 'Neighborhood')
# Dropping Other column
nh = nh.drop('Neighborhood_Other', axis=1)
# Adding result to master df
df = pd.concat([df, nh], axis=1)
# Dropping original variable
df = df.drop('Neighborhood', axis=1)

"""##### Condition1"""

df = df.replace({'Condition1': {'RRNe':'Other', 'RRNn':'Other', 'PosA':'Other', 'RRAe':'Other', 'PosN':'Other',
                               'RRAn':'Other', 'Artery':'Other'}})

# Creating dummy variable
co = pd.get_dummies(df['Condition1'], prefix = 'Condition1')
# Dropping Other column
co = co.drop('Condition1_Other', axis=1)
# Adding result to master df
df = pd.concat([df, co], axis=1)
# Dropping original variable
df = df.drop('Condition1', axis=1)

"""##### HouseStyle"""

df = df.replace({'HouseStyle': {'2.5Fin':'Other', '2.5Unf':'Other', '1.5Unf':'Other', 'SFoyer':'Other', 'SLvl':'Other'}})

# Creating dummy variable
hs = pd.get_dummies(df['HouseStyle'], prefix = 'HouseStyle')
# Dropping Other column
hs = hs.drop('HouseStyle_Other', axis=1)
# Adding result to master df
df = pd.concat([df, hs], axis=1)
# Dropping original variable
df = df.drop('HouseStyle', axis=1)

"""##### OverallQual"""

df = df.replace({'OverallQual': {1:'Other', 2:'Other', 10:'Other', 3:'Other', 9:'Other'}})

# Creating dummy variable
oq = pd.get_dummies(df['OverallQual'], prefix = 'OverallQual')
# Dropping Other column
oq = oq.drop('OverallQual_Other', axis=1)
# Adding result to master df
df = pd.concat([df, oq], axis=1)
# Dropping original variable
df = df.drop('OverallQual', axis=1)

"""##### OverallCond"""

df = df.replace({'OverallCond': {1:'Other', 2:'Other', 9:'Other', 3:'Other', 4:'Other'}})

# Creating dummy variable
oc = pd.get_dummies(df['OverallCond'], prefix = 'OverallCond')
# Dropping Other column
oc = oc.drop('OverallCond_Other', axis=1)
# Adding result to master df
df = pd.concat([df, oc], axis=1)
# Dropping original variable
df = df.drop('OverallCond', axis=1)

"""##### RoofStyle"""

df = df.replace({'RoofStyle': {'Shed':'Other', 'Mansard':'Other', 'Gambrel':'Other', 'Flat':'Other'}})

# Creating dummy variable
rs = pd.get_dummies(df['RoofStyle'], prefix = 'RoofStyle')
# Dropping Other column
rs = rs.drop('RoofStyle_Other', axis=1)
# Adding result to master df
df = pd.concat([df, rs], axis=1)
# Dropping original variable
df = df.drop('RoofStyle', axis=1)

"""##### Exterior1st"""

df = df.replace({'Exterior1st': {'ImStucc':'Other', 'AsphShn':'Other', 'CBlock':'Other', 'BrkComm':'Other','Stone':'Other',
                              'AsbShng':'Other','Stucco':'Other','WdShing':'Other','BrkFace':'Other','CemntBd':'Other'}})

# Creating dummy variable
ex = pd.get_dummies(df['Exterior1st'], prefix = 'Exterior1st')
# Dropping Other column
ex = ex.drop('Exterior1st_Other', axis=1)
# Adding result to master df
df = pd.concat([df, ex], axis=1)
# Dropping original variable
df = df.drop('Exterior1st', axis=1)

"""##### Exterior2nd"""

df = df.replace({'Exterior2nd': {'CBlock':'Other', 'AsphShn':'Other', 'Stone':'Other', 'Brk Cmn':'Other','ImStucc':'Other',
                              'AsbShng':'Other','BrkFace':'Other','Stucco':'Other','Wd Shng':'Other','CmentBd':'Other'}})

# Creating dummy variable
ex2 = pd.get_dummies(df['Exterior2nd'], prefix = 'Exterior2nd')
# Dropping Other column
ex2 = ex2.drop('Exterior2nd_Other', axis=1)
# Adding result to master df
df = pd.concat([df, ex2], axis=1)
# Dropping original variable
df = df.drop('Exterior2nd', axis=1)

"""##### MasVnrType"""

# Creating dummy variable
mv = pd.get_dummies(df['MasVnrType'], prefix = 'MasVnrType')
# Dropping Other column
mv = mv.drop('MasVnrType_BrkCmn', axis=1)
# Adding result to master df
df = pd.concat([df, mv], axis=1)
# Dropping original variable
df = df.drop('MasVnrType', axis=1)

"""##### ExterQual"""

df = df.replace({'ExterQual': {'Fa':'Other', 'Ex':'Other'}})

# Creating dummy variable
exq = pd.get_dummies(df['ExterQual'], prefix = 'ExterQual')
# Dropping Other column
exq = exq.drop('ExterQual_Other', axis=1)
# Adding result to master df
df = pd.concat([df, exq], axis=1)
# Dropping original variable
df = df.drop('ExterQual', axis=1)

"""##### Foundation"""

df = df.replace({'Foundation': {'Wood':'Other', 'Stone':'Other','Slab':'Other'}})

# Creating dummy variable
fo = pd.get_dummies(df['Foundation'], prefix = 'Foundation')
# Dropping Other column
fo = fo.drop('Foundation_Other', axis=1)
# Adding result to master df
df = pd.concat([df, fo], axis=1)
# Dropping original variable
df = df.drop('Foundation', axis=1)

"""##### BsmtQual"""

df = df.replace({'BsmtQual': {'Fa':'Other', 'No Basement':'Other'}})

# Creating dummy variable
bq = pd.get_dummies(df['BsmtQual'], prefix = 'BsmtQual')
# Dropping Other column
bq = bq.drop('BsmtQual_Other', axis=1)
# Adding result to master df
df = pd.concat([df, bq], axis=1)
# Dropping original variable
df = df.drop('BsmtQual', axis=1)

"""##### BsmtExposure"""

# Creating dummy variable
be = pd.get_dummies(df['BsmtExposure'], prefix = 'BsmtExposure')
# Dropping Other column
be = be.drop('BsmtExposure_No Basement', axis=1)
# Adding result to master df
df = pd.concat([df, be], axis=1)
# Dropping original variable
df = df.drop('BsmtExposure', axis=1)

"""##### BsmtFinType1"""

# Creating dummy variable
bf = pd.get_dummies(df['BsmtFinType1'], prefix = 'BsmtFinType1')
# Dropping Other column
bf = bf.drop('BsmtFinType1_No Basement', axis=1)
# Adding result to master df
df = pd.concat([df, bf], axis=1)
# Dropping original variable
df = df.drop('BsmtFinType1', axis=1)

"""##### HeatingQC"""

df = df.replace({'HeatingQC': {'Fa':'Other', 'Po':'Other'}})

# Creating dummy variable
hc = pd.get_dummies(df['HeatingQC'], prefix = 'HeatingQC')
# Dropping Other column
hc = hc.drop('HeatingQC_Other', axis=1)
# Adding result to master df
df = pd.concat([df, hc], axis=1)
# Dropping original variable
df = df.drop('HeatingQC', axis=1)

"""##### KitchenQual"""

df = df.replace({'KitchenQual': {'Fa':'Other', 'Ex':'Other'}})

# Creating dummy variable
kq = pd.get_dummies(df['KitchenQual'], prefix = 'KitchenQual')
# Dropping Other column
kq = kq.drop('KitchenQual_Other', axis=1)
# Adding result to master df
df = pd.concat([df, kq], axis=1)
# Dropping original variable
df = df.drop('KitchenQual', axis=1)

"""##### FireplaceQu"""

df = df.replace({'FireplaceQu': {'Fa':'Other', 'Ex':'Other','Po':'Other'}})

# Creating dummy variable
fq = pd.get_dummies(df['FireplaceQu'], prefix = 'FireplaceQu')
# Dropping Other column
fq = fq.drop('FireplaceQu_Other', axis=1)
# Adding result to master df
df = pd.concat([df, fq], axis=1)
# Dropping original variable
df = df.drop('FireplaceQu', axis=1)

"""##### GarageType"""

df = df.replace({'GarageType': {'2Types':'Other', 'CarPort':'Other','Basment':'Other'}})

# Creating dummy variable
gt = pd.get_dummies(df['GarageType'], prefix = 'GarageType')
# Dropping Other column
gt = gt.drop('GarageType_Other', axis=1)
# Adding result to master df
df = pd.concat([df, gt], axis=1)
# Dropping original variable
df = df.drop('GarageType', axis=1)

"""##### GarageFinish"""

# Creating dummy variable
gf = pd.get_dummies(df['GarageFinish'], prefix = 'GarageFinish')
# Dropping Other column
gf = gf.drop('GarageFinish_No Garage', axis=1)
# Adding result to master df
df = pd.concat([df, gf], axis=1)
# Dropping original variable
df = df.drop('GarageFinish', axis=1)

"""##### Fence"""

df = df.replace({'Fence': {'MnWw':'Other', 'GdWo':'Other','GdPrv':'Other'}})

# Creating dummy variable
fe = pd.get_dummies(df['Fence'], prefix = 'Fence')
# Dropping Other column
fe = fe.drop('Fence_Other', axis=1)
# Adding result to master df
df = pd.concat([df, fe], axis=1)
# Dropping original variable
df = df.drop('Fence', axis=1)

"""##### SaleCondition"""

df = df.replace({'SaleCondition': {'AdjLand':'Other', 'Alloca':'Other','Family':'Other'}})

# Creating dummy variable
se = pd.get_dummies(df['SaleCondition'], prefix = 'SaleCondition')
# Dropping Other column
se = se.drop('SaleCondition_Other', axis=1)
# Adding result to master df
df = pd.concat([df, se], axis=1)
# Dropping original variable
df = df.drop('SaleCondition', axis=1)

"""##### MSSubClass
As we can see that the type of dwelling are represented by their relevant code, we are combining the lower number of counts to an unique code call `100`.
"""

df = df.replace({'MSSubClass': {40:'Other', 180:'Other',45:'Other', 75:'Other',85:'Other',190:'Other',90:'Other',
                               80:'Other',70:'Other',160:'Other',30:'Other'}})

# Creating dummy variable
ms = pd.get_dummies(df['MSSubClass'], prefix = 'MSSubClass')
# Dropping Other column
ms = ms.drop('MSSubClass_Other', axis=1)
# Adding result to master df
df = pd.concat([df, ms], axis=1)
# Dropping original variable
df = df.drop('MSSubClass', axis=1)

df.head()

df.shape

"""We can see that, we have 120 coulmns in the dataset.

### Checking distribution of the target column `SalePrice`
"""

sns.distplot(df['SalePrice'])

# Skewness
df['SalePrice'].skew()

"""We can see that the distribution plot of the SalePrice column is little right skewed.

# Step 2: - Preparing the data for modelling

## Test-Train Split
"""

# Import library
from sklearn.model_selection import train_test_split

# Dropping the Id column
df = df.drop('Id', axis=1)

# Splitting data into train and test set with 80:20 ratio
df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, random_state=100)

print(df_train.shape)
print(df_test.shape)

"""## Feature Scaling"""

# Normalization method
from sklearn.preprocessing import MinMaxScaler

# Instantiate the scaler
scaler = MinMaxScaler()

# Sclaing only the numeric variables. We don't need to scale the binary variables as they are already scaled into 0 and 1
# Numeric columns list
numeric_cols = list(df_train.select_dtypes(include=['int64', 'float64']).columns)
# Fit the data into scaler and transform
df_train[numeric_cols] = scaler.fit_transform(df_train[numeric_cols])
df_train.head()

df_train.describe()

"""### Looking at Correlations"""

# Variables more than 0.70 correlations 
c = df_train.corr().abs()
s = c.unstack()
so = s.sort_values(kind="quicksort", ascending=False)
df_corr = pd.DataFrame(so)
#df_corr.columns = ['correlations']
print(df_corr[(df_corr[0] < 1) & (df_corr[0] > 0.7)])

"""##### Removing one variable from the highly correlated variables pair"""

df_train = df_train.drop(['Exterior2nd_VinylSd','Exterior2nd_MetalSd','HouseStyle_1.5Fin','RoofStyle_Hip','LotShape_IR1',
                         'ExterQual_Gd','Fireplaces','GarageCars','Exterior2nd_HdBoard','Exterior2nd_Wd Sdng',
                         'KitchenQual_Gd','TotRmsAbvGrd','HouseStyle_2Story','MSZoning_RM','MasVnrType_BrkFace',
                         'TotalBsmtSF','Foundation_CBlock','HouseStyle_1Story','BsmtQual_Gd','LotConfig_Corner',
                         'Exterior2nd_Plywood','Fence_MnPrv','GarageType_Detchd'], axis=1)

df_train.shape

"""##### Create X_train and y_train"""

# Popping out the SalePrice column for y_train
y_train = df_train.pop('SalePrice')
# Creating X_train
X_train = df_train

y_train.head()

X_train.head()

# Scale the test set variables with min-max scaler
# We don't fit scaler on the test set. We only transform the test set.
df_test[numeric_cols] = scaler.transform(df_test[numeric_cols]) 
df_test.head()

df_test.describe()

"""##### Create X_test and y_test"""

# popping out the SalePrice column to create y_test
y_test = df_test.pop('SalePrice')
# Creating X_test
X_test = df_test

"""##### We have to drop the variables, which we dropped in the `train set`"""

# Retaining the columns of X_train to X_test
X_test = X_test[X_train.columns]
X_test.head()

"""# Step 3: - Building the model

### Normal Regression without Regularisation
"""

from sklearn.linear_model import LinearRegression
import sklearn.metrics as metrics

# Instantiate the linear regression
lm = LinearRegression()
# Fit the model with train set
lm.fit(X_train, y_train)

# Predict the model with train set
y_train_pred = lm.predict(X_train)
# r2 score for the train model
metrics.r2_score(y_true = y_train, y_pred = y_train_pred)

# Fit the model with test set
y_test_pred = lm.predict(X_test)
# r2 score for the test set
metrics.r2_score(y_true = y_test, y_pred = y_test_pred)

# Model parameters coefficients
model_parameters = lm.coef_
# model coefficients
cols = X_train.columns
cols = cols.insert(0, "constant")
list(zip(cols, model_parameters))

"""## Ridge Regression"""

# Import Ridge regression module, Grid Serach CV and KFold
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge

# Specify the range of hyperparameters (alpha)
params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 
 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 
 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]}

# Instantiate Ridge regression
ridge = Ridge()

# Cross validation with 5 folds
folds = 5
model_cv = GridSearchCV(estimator=ridge,
                       param_grid=params,
                       scoring='r2',
                       cv=folds,
                       return_train_score=True,
                       verbose=1)
# Fitting the model with train set
model_cv.fit(X_train, y_train)

# creating dataframe with model_ridge results
ridge_results = pd.DataFrame(model_cv.cv_results_)
ridge_results.head()

"""##### Plotting the mean of the train and test results """

# Converting the 'param_alpha' datatype from object to int
ridge_results['param_alpha'] = ridge_results['param_alpha'].astype('int32')

# Plotting mean of Train score
plt.figure(figsize=(8,6))
plt.plot(ridge_results['param_alpha'], ridge_results['mean_train_score'])
# Plotting mean of the Test score
plt.plot(ridge_results['param_alpha'], ridge_results['mean_test_score'])
plt.legend(['train score', 'test score'])
plt.xlabel('alpha')
plt.ylabel('mean r2 score')
plt.show()

"""##### Analysis of the above graph

***Train Score***

1. As the alpha (lambda) increases, the r2 score decreases. That means the error increases. Because the model becomes less overfitting and more generalised. Thus by increasing the alpha the model becomes more simple.

***Test Score***

1. With the very lower value of alpha, the error is high as we can see the r2 value decreased. But the error for the train set is low. It means that the model is clearly overfitting with very low value of alpha.
2. With the increasing value of alpha, the error started decreasing more and it reached to a peak at alph=2. Here, the error is least and accuracy (r2 score) is the highest.
3. After alpha=2, the r2 score started decreasing as the alpha is increasing. Hence, the model accuracy started dipping.

`We need to pick the value of aplha for which the test score peaks up.` In this case in alpha=2, the error is least in the test set and hence the accuracy is more approximately 81%.

So, the optimum alpha will be 2, for which we will have a right balance between the error and the generalisation of the model for creting a simpler model.

##### Ridge regression with optimal alpha = 2
"""

# Instantiate Ridge regression with alpha=2
model_ridge = Ridge(alpha=2)
# Fitting the model with the train set
model_ridge.fit(X_train, y_train)

"""### Model evaluation Ridge Regression

##### Model performance on the train set
"""

y_train_pred = model_ridge.predict(X_train)
print(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))

"""##### Model performance on the test set"""

y_test_pred = model_ridge.predict(X_test)
print(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))

"""#### Conclusion

We have a good train score 85% and good test score as well 83%. That means what the model learnt in the train set it performed well in the test set.

#### Ridge Regression Model Parameters
"""

# Coefficients list
model_ridge_parameters = list(model_ridge.coef_)
# Inserting Y Intercept to model parameters list
model_ridge_parameters.insert(0, model_ridge.intercept_)
# Rounding off the coefficients
model_ridge_parameters = [round(i,3) for i in model_ridge_parameters]

cols = X_train.columns
cols = cols.insert(0,'constant')
ridge_param_list = list(zip(cols, model_ridge_parameters))
ridge_param_list

"""##### Top 10 features of Ridge regression"""

ridge_params_df = pd.DataFrame({'Params':cols, 'Coef':model_ridge_parameters})
(ridge_params_df.sort_values('Coef', ascending=False)).head(11)

"""## Lasso Regression"""

# Importing Lasso module
from sklearn.linear_model import Lasso

params = {'alpha': [0.000001,0.00001,0.0001,0.001, 0.01, 1.0,2]}

# Instantiate Lasso regression
lasso = Lasso()

# Cross validation with 5 folds
model_cv = GridSearchCV(estimator=lasso,
                       param_grid=params,
                       scoring='r2',
                       cv=folds,
                       return_train_score=True,
                       verbose=1)
# Fitting the model with train set
model_cv.fit(X_train, y_train)

# creating dataframe with model_cv results
lasso_results = pd.DataFrame(model_cv.cv_results_)
lasso_results.head()

"""##### Plotting the mean of the train and test results """

# Converting the 'param_alpha' datatype from object to int
lasso_results['param_alpha'] = lasso_results['param_alpha'].astype('int32')

# Plotting mean of Train score
plt.figure(figsize=(8,6))
plt.plot(lasso_results['param_alpha'], lasso_results['mean_train_score'])
# Plotting mean of the Test score
plt.plot(lasso_results['param_alpha'], lasso_results['mean_test_score'])

plt.legend(['train score', 'test score'])
plt.xlabel('alpha')
plt.ylabel('mean r2 score')
plt.show()

"""#### Analysis of the above graph

From the above graph we can see that with very lower value of alpha (almost colse to 0) the accuracy of the train and test set is the highest. Coincidentally they both are alomost same.

***Train Score***

1. As the alpha (lambda) increases, the r2 score decreases. That means the error increases. Because the model becomes less overfitting and more generalised. At 0.002 (close to 0) the train set accury is highest(more than 80%).

***Test Score***

1. At alpha = 0.002 the test accuracy is highest (more than 80%).
3. After alpha=0.002, the r2 score started decreasing as the alpha is increasing. Hence, the model accuracy started dipping.

`We need to pick the value of aplha for which the test score peaks up.` In this case at alpha=0.002, the error is least in the test set and hence the accuracy is more approximately 80%.

So, the optimum alpha will be 0.002, for which we will have a right balance between the error and the generalisation of the model for creating a simpler model.

##### Lasso regression with optimal alpha 0.002
"""

# Instantiate Lasso regression with alpha=0.002
model_lasso = Lasso(0.002)
# Fitting the model with the train set
model_lasso.fit(X_train, y_train)

"""### Model evaluation Lasso Regression

##### Model performance on the train set
"""

y_train_pred = model_lasso.predict(X_train)
print(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))

"""##### Model performance on the test set"""

y_test_pred = model_lasso.predict(X_test)
print(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))

"""#### Conclusion

We have train set accuracy 75% whereas test set accuracy is 70%. From the accuracy gap, we can conclude that what the model learnt on the train set, it performed well on the test set.

#### Lasso Regression Model Parameters
"""

# Coefficients list
model_lasso_parameters = list(model_lasso.coef_)
# Inserting Y Intercept to model parameters list
model_lasso_parameters.insert(0, model_lasso.intercept_)
# Rounding off the coefficients
model_lasso_parameters = [round(i,3) for i in model_lasso_parameters]

cols = X_train.columns
cols = cols.insert(0,'constant')
lasso_param_list = list(zip(cols, model_lasso_parameters))
lasso_param_list

"""#### Feature selected by Lasso regression"""

# Parameters having coefficients greater than 1
lasso_params_df = pd.DataFrame({'Params':cols, 'Coef':model_lasso_parameters})
lasso_params_df = lasso_params_df.loc[lasso_params_df['Coef'] != 0]
lasso_params_df

"""##### Number of parameters selected by the Lasso regression"""

# Minus one as we have constant present in lasso_params_df
len(lasso_params_df)-1

# Sorting the parameters with their Coefficient values
lasso_params_df.sort_values('Coef',ascending=False)
